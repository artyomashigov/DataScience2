{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition has been launched, please register using this [link.](https://www.kaggle.com/t/f79b637ede074e70a233661b4614083c)\n",
    "\n",
    "You will find the training and test data in the data section of the competition, along with a description of the features. You will need to build models on the training data and make predictions on the test data and submit your solutions to Kaggle. You will also find a sample solution file in the data section that shows the format you will need to use for your own submissions.\n",
    "\n",
    "The deadline for Kaggle solutions is 8PM on 19 April. You will be graded primarily on the basis of your work and how clearly you explain your methods and results. Those in the top three in the competition will receive some extra points. I expect you to experiment with all the methods we have covered: linear models, random forest, gradient boosting, neural networks + parameter tuning, feature engineering.\n",
    "\n",
    "You will see the public score of your best model on the leaderboard. A private dataset will be used to evaluate the final performance of your model to avoid overfitting based on the leaderboard.\n",
    "\n",
    "You should also submit to Moodle the documentation (ipynb and pdf) of your work, including exploratory data analysis, data cleaning, parameter tuning and evaluation. Aim for concise explanations.\n",
    "\n",
    "Feel free to ask questions about the task in Slack. The Kaggle competition is already open, please start working on it and submitting solutions (you cannot submit more than 5 solutions per day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plan\n",
    "\n",
    "Our plan for the Kaggle competition involves a systematic approach to model development and optimization. Initially, we will split the provided 'train' dataframe into two segments to serve as our training and validation sets. We intend to build and evaluate different predictive models—such as logistic regression, random forests, and gradient boosting machines—focusing on the AUC metric to determine their performance. The model with the highest AUC on our validation set will be selected for further refinement. After the initial modeling phase, we will engage in feature engineering to enhance the dataset, followed by rebuilding and re-evaluating the models with the new features. The best-performing model from this second phase will then be applied to the external validation set. Finally, we will prepare and submit our predictions in the required format (article_id and score) to Kaggle, ensuring they align with the competition's guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# show max columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29733, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594</td>\n",
       "      <td>9</td>\n",
       "      <td>702</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.620438</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.790598</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>753.875</td>\n",
       "      <td>2900</td>\n",
       "      <td>690400</td>\n",
       "      <td>194850.0</td>\n",
       "      <td>2635.188119</td>\n",
       "      <td>5109.090909</td>\n",
       "      <td>3411.008319</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3850.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453017</td>\n",
       "      <td>0.025031</td>\n",
       "      <td>0.025053</td>\n",
       "      <td>0.025027</td>\n",
       "      <td>0.471872</td>\n",
       "      <td>0.499743</td>\n",
       "      <td>0.224620</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.452328</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153395</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>8</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.470143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666209</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4.622389</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>417.0</td>\n",
       "      <td>120.000</td>\n",
       "      <td>0</td>\n",
       "      <td>843300</td>\n",
       "      <td>221250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4221.436364</td>\n",
       "      <td>2393.577920</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>68300.0</td>\n",
       "      <td>21600.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.034222</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.865673</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.509674</td>\n",
       "      <td>0.169761</td>\n",
       "      <td>0.060986</td>\n",
       "      <td>0.020886</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308167</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484</td>\n",
       "      <td>9</td>\n",
       "      <td>214</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.476636</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>512.0</td>\n",
       "      <td>138.625</td>\n",
       "      <td>2400</td>\n",
       "      <td>843300</td>\n",
       "      <td>184962.5</td>\n",
       "      <td>945.500000</td>\n",
       "      <td>3602.455629</td>\n",
       "      <td>2481.347103</td>\n",
       "      <td>331.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.899229</td>\n",
       "      <td>0.311717</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.254562</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639</td>\n",
       "      <td>8</td>\n",
       "      <td>249</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664740</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.180723</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>717.0</td>\n",
       "      <td>146.600</td>\n",
       "      <td>0</td>\n",
       "      <td>617900</td>\n",
       "      <td>221150.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4612.708333</td>\n",
       "      <td>2920.744778</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>5866.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.373002</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.526997</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.387522</td>\n",
       "      <td>-0.006684</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.231818</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177</td>\n",
       "      <td>12</td>\n",
       "      <td>1219</td>\n",
       "      <td>0.397841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583578</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.659557</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.250</td>\n",
       "      <td>3100</td>\n",
       "      <td>843300</td>\n",
       "      <td>263875.0</td>\n",
       "      <td>844.562500</td>\n",
       "      <td>3529.366510</td>\n",
       "      <td>2250.688809</td>\n",
       "      <td>627.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.799995</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.457970</td>\n",
       "      <td>0.077511</td>\n",
       "      <td>0.025431</td>\n",
       "      <td>0.012305</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.380401</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.441111</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        594               9               702         0.454545   \n",
       "1        346               8              1197         0.470143   \n",
       "2        484               9               214         0.618090   \n",
       "3        639               8               249         0.621951   \n",
       "4        177              12              1219         0.397841   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.620438         11               2   \n",
       "1               1.0                  0.666209         21               6   \n",
       "2               1.0                  0.748092          5               2   \n",
       "3               1.0                  0.664740         16               5   \n",
       "4               1.0                  0.583578         21               1   \n",
       "\n",
       "   num_imgs  num_videos  average_token_length  num_keywords  \\\n",
       "0         1           0              4.790598             8   \n",
       "1         2          13              4.622389             6   \n",
       "2         1           0              4.476636             8   \n",
       "3         8           0              5.180723             6   \n",
       "4         1           2              4.659557             4   \n",
       "\n",
       "   data_channel_is_lifestyle  data_channel_is_entertainment  \\\n",
       "0                          0                              0   \n",
       "1                          0                              1   \n",
       "2                          0                              0   \n",
       "3                          0                              0   \n",
       "4                          0                              0   \n",
       "\n",
       "   data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  \\\n",
       "0                    1                       0                     0   \n",
       "1                    0                       0                     0   \n",
       "2                    0                       0                     1   \n",
       "3                    0                       0                     0   \n",
       "4                    0                       0                     0   \n",
       "\n",
       "   data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  \\\n",
       "0                      0           4      2900.0     753.875        2900   \n",
       "1                      0          -1       417.0     120.000           0   \n",
       "2                      0           4       512.0     138.625        2400   \n",
       "3                      0           4       717.0     146.600           0   \n",
       "4                      1          -1        60.0      25.250        3100   \n",
       "\n",
       "   kw_max_max  kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0      690400    194850.0  2635.188119  5109.090909  3411.008319   \n",
       "1      843300    221250.0     0.000000  4221.436364  2393.577920   \n",
       "2      843300    184962.5   945.500000  3602.455629  2481.347103   \n",
       "3      617900    221150.0     0.000000  4612.708333  2920.744778   \n",
       "4      843300    263875.0   844.562500  3529.366510  2250.688809   \n",
       "\n",
       "   self_reference_min_shares  self_reference_max_shares  \\\n",
       "0                     1200.0                     6500.0   \n",
       "1                     1900.0                    68300.0   \n",
       "2                      331.0                      331.0   \n",
       "3                     2900.0                     8800.0   \n",
       "4                      627.0                      627.0   \n",
       "\n",
       "   self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  \\\n",
       "0                 3850.000000                  0                   0   \n",
       "1                21600.000000                  1                   0   \n",
       "2                  331.000000                  0                   0   \n",
       "3                 5866.666667                  0                   1   \n",
       "4                  627.000000                  0                   1   \n",
       "\n",
       "   weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  \\\n",
       "0                     0                    0                  1   \n",
       "1                     0                    0                  0   \n",
       "2                     1                    0                  0   \n",
       "3                     0                    0                  0   \n",
       "4                     0                    0                  0   \n",
       "\n",
       "   weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01  \\\n",
       "0                    0                  0           0  0.453017  0.025031   \n",
       "1                    0                  0           0  0.033338  0.034222   \n",
       "2                    0                  0           0  0.025767  0.025002   \n",
       "3                    0                  0           0  0.033333  0.373002   \n",
       "4                    0                  0           0  0.050001  0.050000   \n",
       "\n",
       "     LDA_02    LDA_03    LDA_04  global_subjectivity  \\\n",
       "0  0.025053  0.025027  0.471872             0.499743   \n",
       "1  0.033336  0.865673  0.033430             0.509674   \n",
       "2  0.025002  0.025000  0.899229             0.311717   \n",
       "3  0.033333  0.526997  0.033333             0.387522   \n",
       "4  0.799995  0.050000  0.050004             0.457970   \n",
       "\n",
       "   global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0                   0.224620                    0.047009   \n",
       "1                   0.169761                    0.060986   \n",
       "2                   0.060404                    0.042056   \n",
       "3                  -0.006684                    0.036145   \n",
       "4                   0.077511                    0.025431   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.012821             0.785714             0.214286   \n",
       "1                    0.020886             0.744898             0.255102   \n",
       "2                    0.028037             0.600000             0.400000   \n",
       "3                    0.016064             0.692308             0.307692   \n",
       "4                    0.012305             0.673913             0.326087   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.452328                   0.10               1.000000   \n",
       "1               0.434561                   0.05               1.000000   \n",
       "2               0.254562                   0.10               0.433333   \n",
       "3               0.231818                   0.10               0.500000   \n",
       "4               0.380401                   0.05               0.800000   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.153395                   -0.4                  -0.10   \n",
       "1              -0.308167                   -1.0                  -0.10   \n",
       "2              -0.141667                   -0.2                  -0.05   \n",
       "3              -0.500000                   -0.8                  -0.40   \n",
       "4              -0.441111                   -1.0                  -0.05   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 0.0                       0.0                     0.5   \n",
       "1                 0.0                       0.0                     0.5   \n",
       "2                 0.0                       0.0                     0.5   \n",
       "3                 0.0                       0.0                     0.5   \n",
       "4                 0.0                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  is_popular  article_id  \n",
       "0                           0.0           0           1  \n",
       "1                           0.0           0           3  \n",
       "2                           0.0           0           5  \n",
       "3                           0.0           0           6  \n",
       "4                           0.0           0           7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "# check the shape\n",
    "print(train.shape)\n",
    "# print first 5 rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29733 entries, 0 to 29732\n",
      "Data columns (total 61 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   timedelta                      29733 non-null  int64  \n",
      " 1   n_tokens_title                 29733 non-null  int64  \n",
      " 2   n_tokens_content               29733 non-null  int64  \n",
      " 3   n_unique_tokens                29733 non-null  float64\n",
      " 4   n_non_stop_words               29733 non-null  float64\n",
      " 5   n_non_stop_unique_tokens       29733 non-null  float64\n",
      " 6   num_hrefs                      29733 non-null  int64  \n",
      " 7   num_self_hrefs                 29733 non-null  int64  \n",
      " 8   num_imgs                       29733 non-null  int64  \n",
      " 9   num_videos                     29733 non-null  int64  \n",
      " 10  average_token_length           29733 non-null  float64\n",
      " 11  num_keywords                   29733 non-null  int64  \n",
      " 12  data_channel_is_lifestyle      29733 non-null  int64  \n",
      " 13  data_channel_is_entertainment  29733 non-null  int64  \n",
      " 14  data_channel_is_bus            29733 non-null  int64  \n",
      " 15  data_channel_is_socmed         29733 non-null  int64  \n",
      " 16  data_channel_is_tech           29733 non-null  int64  \n",
      " 17  data_channel_is_world          29733 non-null  int64  \n",
      " 18  kw_min_min                     29733 non-null  int64  \n",
      " 19  kw_max_min                     29733 non-null  float64\n",
      " 20  kw_avg_min                     29733 non-null  float64\n",
      " 21  kw_min_max                     29733 non-null  int64  \n",
      " 22  kw_max_max                     29733 non-null  int64  \n",
      " 23  kw_avg_max                     29733 non-null  float64\n",
      " 24  kw_min_avg                     29733 non-null  float64\n",
      " 25  kw_max_avg                     29733 non-null  float64\n",
      " 26  kw_avg_avg                     29733 non-null  float64\n",
      " 27  self_reference_min_shares      29733 non-null  float64\n",
      " 28  self_reference_max_shares      29733 non-null  float64\n",
      " 29  self_reference_avg_sharess     29733 non-null  float64\n",
      " 30  weekday_is_monday              29733 non-null  int64  \n",
      " 31  weekday_is_tuesday             29733 non-null  int64  \n",
      " 32  weekday_is_wednesday           29733 non-null  int64  \n",
      " 33  weekday_is_thursday            29733 non-null  int64  \n",
      " 34  weekday_is_friday              29733 non-null  int64  \n",
      " 35  weekday_is_saturday            29733 non-null  int64  \n",
      " 36  weekday_is_sunday              29733 non-null  int64  \n",
      " 37  is_weekend                     29733 non-null  int64  \n",
      " 38  LDA_00                         29733 non-null  float64\n",
      " 39  LDA_01                         29733 non-null  float64\n",
      " 40  LDA_02                         29733 non-null  float64\n",
      " 41  LDA_03                         29733 non-null  float64\n",
      " 42  LDA_04                         29733 non-null  float64\n",
      " 43  global_subjectivity            29733 non-null  float64\n",
      " 44  global_sentiment_polarity      29733 non-null  float64\n",
      " 45  global_rate_positive_words     29733 non-null  float64\n",
      " 46  global_rate_negative_words     29733 non-null  float64\n",
      " 47  rate_positive_words            29733 non-null  float64\n",
      " 48  rate_negative_words            29733 non-null  float64\n",
      " 49  avg_positive_polarity          29733 non-null  float64\n",
      " 50  min_positive_polarity          29733 non-null  float64\n",
      " 51  max_positive_polarity          29733 non-null  float64\n",
      " 52  avg_negative_polarity          29733 non-null  float64\n",
      " 53  min_negative_polarity          29733 non-null  float64\n",
      " 54  max_negative_polarity          29733 non-null  float64\n",
      " 55  title_subjectivity             29733 non-null  float64\n",
      " 56  title_sentiment_polarity       29733 non-null  float64\n",
      " 57  abs_title_subjectivity         29733 non-null  float64\n",
      " 58  abs_title_sentiment_polarity   29733 non-null  float64\n",
      " 59  is_popular                     29733 non-null  int64  \n",
      " 60  article_id                     29733 non-null  int64  \n",
      "dtypes: float64(34), int64(27)\n",
      "memory usage: 13.8 MB\n"
     ]
    }
   ],
   "source": [
    "# check info\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently we don't have any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>355.645646</td>\n",
       "      <td>10.390812</td>\n",
       "      <td>545.008274</td>\n",
       "      <td>0.555076</td>\n",
       "      <td>1.005852</td>\n",
       "      <td>0.695432</td>\n",
       "      <td>10.912690</td>\n",
       "      <td>3.290788</td>\n",
       "      <td>4.524535</td>\n",
       "      <td>1.263546</td>\n",
       "      <td>4.551856</td>\n",
       "      <td>7.230787</td>\n",
       "      <td>0.053173</td>\n",
       "      <td>0.177379</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.183702</td>\n",
       "      <td>0.213467</td>\n",
       "      <td>26.272626</td>\n",
       "      <td>1128.306988</td>\n",
       "      <td>310.641686</td>\n",
       "      <td>13498.828776</td>\n",
       "      <td>751395.540309</td>\n",
       "      <td>258836.020623</td>\n",
       "      <td>1112.463476</td>\n",
       "      <td>5650.631582</td>\n",
       "      <td>3131.942389</td>\n",
       "      <td>4034.177470</td>\n",
       "      <td>10361.528395</td>\n",
       "      <td>6450.838297</td>\n",
       "      <td>0.167558</td>\n",
       "      <td>0.187435</td>\n",
       "      <td>0.187401</td>\n",
       "      <td>0.183836</td>\n",
       "      <td>0.142972</td>\n",
       "      <td>0.062590</td>\n",
       "      <td>0.068207</td>\n",
       "      <td>0.130797</td>\n",
       "      <td>0.183942</td>\n",
       "      <td>0.141270</td>\n",
       "      <td>0.215628</td>\n",
       "      <td>0.225164</td>\n",
       "      <td>0.233963</td>\n",
       "      <td>0.443645</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.039665</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.683187</td>\n",
       "      <td>0.287485</td>\n",
       "      <td>0.354020</td>\n",
       "      <td>0.095593</td>\n",
       "      <td>0.757780</td>\n",
       "      <td>-0.259709</td>\n",
       "      <td>-0.520981</td>\n",
       "      <td>-0.107793</td>\n",
       "      <td>0.281878</td>\n",
       "      <td>0.069691</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>19834.913530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.288261</td>\n",
       "      <td>2.110135</td>\n",
       "      <td>469.358037</td>\n",
       "      <td>4.064572</td>\n",
       "      <td>6.039655</td>\n",
       "      <td>3.768796</td>\n",
       "      <td>11.316508</td>\n",
       "      <td>3.840874</td>\n",
       "      <td>8.213823</td>\n",
       "      <td>4.189080</td>\n",
       "      <td>0.836817</td>\n",
       "      <td>1.910960</td>\n",
       "      <td>0.224383</td>\n",
       "      <td>0.381995</td>\n",
       "      <td>0.364089</td>\n",
       "      <td>0.233014</td>\n",
       "      <td>0.387247</td>\n",
       "      <td>0.409761</td>\n",
       "      <td>69.818552</td>\n",
       "      <td>3242.275908</td>\n",
       "      <td>592.878716</td>\n",
       "      <td>58263.629521</td>\n",
       "      <td>215516.521138</td>\n",
       "      <td>135395.605303</td>\n",
       "      <td>1134.154588</td>\n",
       "      <td>5950.220056</td>\n",
       "      <td>1313.776104</td>\n",
       "      <td>20257.804432</td>\n",
       "      <td>41485.281920</td>\n",
       "      <td>24725.713275</td>\n",
       "      <td>0.373480</td>\n",
       "      <td>0.390267</td>\n",
       "      <td>0.390240</td>\n",
       "      <td>0.387357</td>\n",
       "      <td>0.350051</td>\n",
       "      <td>0.242229</td>\n",
       "      <td>0.252105</td>\n",
       "      <td>0.337184</td>\n",
       "      <td>0.262189</td>\n",
       "      <td>0.220079</td>\n",
       "      <td>0.281680</td>\n",
       "      <td>0.295832</td>\n",
       "      <td>0.289046</td>\n",
       "      <td>0.116106</td>\n",
       "      <td>0.096757</td>\n",
       "      <td>0.017421</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>0.155843</td>\n",
       "      <td>0.104266</td>\n",
       "      <td>0.071503</td>\n",
       "      <td>0.247293</td>\n",
       "      <td>0.128488</td>\n",
       "      <td>0.290454</td>\n",
       "      <td>0.095672</td>\n",
       "      <td>0.323461</td>\n",
       "      <td>0.264379</td>\n",
       "      <td>0.188735</td>\n",
       "      <td>0.225066</td>\n",
       "      <td>0.326886</td>\n",
       "      <td>11432.376037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.377657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.479167</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>141.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>172040.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3560.441280</td>\n",
       "      <td>2381.502070</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>986.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025046</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.026284</td>\n",
       "      <td>0.027857</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.396296</td>\n",
       "      <td>0.058031</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.306172</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328704</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9965.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>342.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690566</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>662.000000</td>\n",
       "      <td>236.333333</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>243857.142857</td>\n",
       "      <td>1017.857143</td>\n",
       "      <td>4351.343264</td>\n",
       "      <td>2866.223520</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040845</td>\n",
       "      <td>0.453674</td>\n",
       "      <td>0.119458</td>\n",
       "      <td>0.039039</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.358784</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.252827</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19859.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>545.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.856373</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>358.285714</td>\n",
       "      <td>7700.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>330660.000000</td>\n",
       "      <td>2047.903286</td>\n",
       "      <td>6015.988776</td>\n",
       "      <td>3593.868442</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>5171.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240513</td>\n",
       "      <td>0.150553</td>\n",
       "      <td>0.333049</td>\n",
       "      <td>0.379922</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.508013</td>\n",
       "      <td>0.178127</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.411955</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186494</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>158900.000000</td>\n",
       "      <td>39979.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>3610.124972</td>\n",
       "      <td>237966.666667</td>\n",
       "      <td>37607.521654</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926994</td>\n",
       "      <td>0.919976</td>\n",
       "      <td>0.919999</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.927191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.162037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39643.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  29733.000000    29733.000000      29733.000000     29733.000000   \n",
       "mean     355.645646       10.390812        545.008274         0.555076   \n",
       "std      214.288261        2.110135        469.358037         4.064572   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.471400   \n",
       "50%      342.000000       10.000000        409.000000         0.539894   \n",
       "75%      545.000000       12.000000        712.000000         0.609375   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      29733.000000              29733.000000  29733.000000   \n",
       "mean           1.005852                  0.695432     10.912690   \n",
       "std            6.039655                  3.768796     11.316508   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.626126      4.000000   \n",
       "50%            1.000000                  0.690566      8.000000   \n",
       "75%            1.000000                  0.755208     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos  average_token_length  \\\n",
       "count    29733.000000  29733.000000  29733.000000          29733.000000   \n",
       "mean         3.290788      4.524535      1.263546              4.551856   \n",
       "std          3.840874      8.213823      4.189080              0.836817   \n",
       "min          0.000000      0.000000      0.000000              0.000000   \n",
       "25%          1.000000      1.000000      0.000000              4.479167   \n",
       "50%          2.000000      1.000000      0.000000              4.666667   \n",
       "75%          4.000000      4.000000      1.000000              4.856373   \n",
       "max         74.000000    111.000000     91.000000              8.041534   \n",
       "\n",
       "       num_keywords  data_channel_is_lifestyle  data_channel_is_entertainment  \\\n",
       "count  29733.000000               29733.000000                   29733.000000   \n",
       "mean       7.230787                   0.053173                       0.177379   \n",
       "std        1.910960                   0.224383                       0.381995   \n",
       "min        1.000000                   0.000000                       0.000000   \n",
       "25%        6.000000                   0.000000                       0.000000   \n",
       "50%        7.000000                   0.000000                       0.000000   \n",
       "75%        9.000000                   0.000000                       0.000000   \n",
       "max       10.000000                   1.000000                       1.000000   \n",
       "\n",
       "       data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  \\\n",
       "count         29733.000000            29733.000000          29733.000000   \n",
       "mean              0.157300                0.057613              0.183702   \n",
       "std               0.364089                0.233014              0.387247   \n",
       "min               0.000000                0.000000              0.000000   \n",
       "25%               0.000000                0.000000              0.000000   \n",
       "50%               0.000000                0.000000              0.000000   \n",
       "75%               0.000000                0.000000              0.000000   \n",
       "max               1.000000                1.000000              1.000000   \n",
       "\n",
       "       data_channel_is_world    kw_min_min     kw_max_min    kw_avg_min  \\\n",
       "count           29733.000000  29733.000000   29733.000000  29733.000000   \n",
       "mean                0.213467     26.272626    1128.306988    310.641686   \n",
       "std                 0.409761     69.818552    3242.275908    592.878716   \n",
       "min                 0.000000     -1.000000       0.000000     -1.000000   \n",
       "25%                 0.000000     -1.000000     445.000000    141.800000   \n",
       "50%                 0.000000     -1.000000     662.000000    236.333333   \n",
       "75%                 0.000000      4.000000    1000.000000    358.285714   \n",
       "max                 1.000000    377.000000  158900.000000  39979.000000   \n",
       "\n",
       "          kw_min_max     kw_max_max     kw_avg_max    kw_min_avg  \\\n",
       "count   29733.000000   29733.000000   29733.000000  29733.000000   \n",
       "mean    13498.828776  751395.540309  258836.020623   1112.463476   \n",
       "std     58263.629521  215516.521138  135395.605303   1134.154588   \n",
       "min         0.000000       0.000000       0.000000     -1.000000   \n",
       "25%         0.000000  843300.000000  172040.000000      0.000000   \n",
       "50%      1400.000000  843300.000000  243857.142857   1017.857143   \n",
       "75%      7700.000000  843300.000000  330660.000000   2047.903286   \n",
       "max    843300.000000  843300.000000  843300.000000   3610.124972   \n",
       "\n",
       "          kw_max_avg    kw_avg_avg  self_reference_min_shares  \\\n",
       "count   29733.000000  29733.000000               29733.000000   \n",
       "mean     5650.631582   3131.942389                4034.177470   \n",
       "std      5950.220056   1313.776104               20257.804432   \n",
       "min         0.000000      0.000000                   0.000000   \n",
       "25%      3560.441280   2381.502070                 641.000000   \n",
       "50%      4351.343264   2866.223520                1200.000000   \n",
       "75%      6015.988776   3593.868442                2700.000000   \n",
       "max    237966.666667  37607.521654              843300.000000   \n",
       "\n",
       "       self_reference_max_shares  self_reference_avg_sharess  \\\n",
       "count               29733.000000                29733.000000   \n",
       "mean                10361.528395                 6450.838297   \n",
       "std                 41485.281920                24725.713275   \n",
       "min                     0.000000                    0.000000   \n",
       "25%                  1100.000000                  986.000000   \n",
       "50%                  2800.000000                 2200.000000   \n",
       "75%                  7900.000000                 5171.250000   \n",
       "max                843300.000000               843300.000000   \n",
       "\n",
       "       weekday_is_monday  weekday_is_tuesday  weekday_is_wednesday  \\\n",
       "count       29733.000000        29733.000000          29733.000000   \n",
       "mean            0.167558            0.187435              0.187401   \n",
       "std             0.373480            0.390267              0.390240   \n",
       "min             0.000000            0.000000              0.000000   \n",
       "25%             0.000000            0.000000              0.000000   \n",
       "50%             0.000000            0.000000              0.000000   \n",
       "75%             0.000000            0.000000              0.000000   \n",
       "max             1.000000            1.000000              1.000000   \n",
       "\n",
       "       weekday_is_thursday  weekday_is_friday  weekday_is_saturday  \\\n",
       "count         29733.000000       29733.000000         29733.000000   \n",
       "mean              0.183836           0.142972             0.062590   \n",
       "std               0.387357           0.350051             0.242229   \n",
       "min               0.000000           0.000000             0.000000   \n",
       "25%               0.000000           0.000000             0.000000   \n",
       "50%               0.000000           0.000000             0.000000   \n",
       "75%               0.000000           0.000000             0.000000   \n",
       "max               1.000000           1.000000             1.000000   \n",
       "\n",
       "       weekday_is_sunday    is_weekend        LDA_00        LDA_01  \\\n",
       "count       29733.000000  29733.000000  29733.000000  29733.000000   \n",
       "mean            0.068207      0.130797      0.183942      0.141270   \n",
       "std             0.252105      0.337184      0.262189      0.220079   \n",
       "min             0.000000      0.000000      0.000000      0.000000   \n",
       "25%             0.000000      0.000000      0.025046      0.025011   \n",
       "50%             0.000000      0.000000      0.033386      0.033345   \n",
       "75%             0.000000      0.000000      0.240513      0.150553   \n",
       "max             1.000000      1.000000      0.926994      0.919976   \n",
       "\n",
       "             LDA_02        LDA_03        LDA_04  global_subjectivity  \\\n",
       "count  29733.000000  29733.000000  29733.000000         29733.000000   \n",
       "mean       0.215628      0.225164      0.233963             0.443645   \n",
       "std        0.281680      0.295832      0.289046             0.116106   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.026284      0.027857      0.028574             0.396296   \n",
       "50%        0.040003      0.040001      0.040845             0.453674   \n",
       "75%        0.333049      0.379922      0.399992             0.508013   \n",
       "max        0.919999      0.926534      0.927191             1.000000   \n",
       "\n",
       "       global_sentiment_polarity  global_rate_positive_words  \\\n",
       "count               29733.000000                29733.000000   \n",
       "mean                    0.119506                    0.039665   \n",
       "std                     0.096757                    0.017421   \n",
       "min                    -0.377657                    0.000000   \n",
       "25%                     0.058031                    0.028455   \n",
       "50%                     0.119458                    0.039039   \n",
       "75%                     0.178127                    0.050279   \n",
       "max                     0.727841                    0.155488   \n",
       "\n",
       "       global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "count                29733.000000         29733.000000         29733.000000   \n",
       "mean                     0.016562             0.683187             0.287485   \n",
       "std                      0.010732             0.189453             0.155843   \n",
       "min                      0.000000             0.000000             0.000000   \n",
       "25%                      0.009615             0.600000             0.185185   \n",
       "50%                      0.015278             0.712121             0.278689   \n",
       "75%                      0.021739             0.800000             0.383721   \n",
       "max                      0.162037             1.000000             1.000000   \n",
       "\n",
       "       avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "count           29733.000000           29733.000000           29733.000000   \n",
       "mean                0.354020               0.095593               0.757780   \n",
       "std                 0.104266               0.071503               0.247293   \n",
       "min                 0.000000               0.000000               0.000000   \n",
       "25%                 0.306172               0.050000               0.600000   \n",
       "50%                 0.358784               0.100000               0.800000   \n",
       "75%                 0.411955               0.100000               1.000000   \n",
       "max                 1.000000               1.000000               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "count           29733.000000           29733.000000           29733.000000   \n",
       "mean               -0.259709              -0.520981              -0.107793   \n",
       "std                 0.128488               0.290454               0.095672   \n",
       "min                -1.000000              -1.000000              -1.000000   \n",
       "25%                -0.328704              -0.700000              -0.125000   \n",
       "50%                -0.252827              -0.500000              -0.100000   \n",
       "75%                -0.186494              -0.300000              -0.050000   \n",
       "max                 0.000000               0.000000               0.000000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count        29733.000000              29733.000000            29733.000000   \n",
       "mean             0.281878                  0.069691                0.341427   \n",
       "std              0.323461                  0.264379                0.188735   \n",
       "min              0.000000                 -1.000000                0.000000   \n",
       "25%              0.000000                  0.000000                0.166667   \n",
       "50%              0.144444                  0.000000                0.500000   \n",
       "75%              0.500000                  0.136364                0.500000   \n",
       "max              1.000000                  1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity    is_popular    article_id  \n",
       "count                  29733.000000  29733.000000  29733.000000  \n",
       "mean                       0.155234      0.121649  19834.913530  \n",
       "std                        0.225066      0.326886  11432.376037  \n",
       "min                        0.000000      0.000000      1.000000  \n",
       "25%                        0.000000      0.000000   9965.000000  \n",
       "50%                        0.000000      0.000000  19859.000000  \n",
       "75%                        0.250000      0.000000  29742.000000  \n",
       "max                        1.000000      1.000000  39643.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Models\n",
    "\n",
    "- Logistic Regression\n",
    "- Lasso Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting Machine (GBM)\n",
    "- Neural Network\n",
    "- Explainable Boosting Machine (EBM)\n",
    "- Support Vector Machine (SVM)\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  AUC Train  AUC Validation\n",
       "0  Model Logistic A1   0.592374        0.599277\n",
       "1  Model Logistic A2   0.615253        0.623459\n",
       "2  Model Logistic A3   0.681887        0.679263\n",
       "3  Model Logistic A4   0.685753        0.680492\n",
       "4  Model Logistic A5   0.686585        0.683885\n",
       "5  Model Logistic A6   0.690455        0.679908\n",
       "6  Model Logistic A7   0.691716        0.681179\n",
       "7  Model Logistic A8   0.695629        0.680625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a random state\n",
    "prng = np.random.RandomState(20240418)\n",
    "\n",
    "# define the traget and features\n",
    "X = train.drop(['is_popular','article_id','timedelta'], axis=1)\n",
    "y = train['is_popular']\n",
    "\n",
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=prng)\n",
    "\n",
    "# define feature groups\n",
    "feature_groups = {\n",
    "    'Basic Article Info': ['n_tokens_title', 'n_tokens_content', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos'],\n",
    "    'Content Quality': ['n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'average_token_length'],\n",
    "    'Engagement Metrics': ['num_keywords', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg'],\n",
    "    'Reference Metrics': ['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess'],\n",
    "    'Publishing Details': ['weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend'],\n",
    "    'Content Analysis': ['LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words',\n",
    "                          'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity', 'avg_negative_polarity', 'min_negative_polarity',\n",
    "                            'max_negative_polarity'],\n",
    "    'Title Analysis': ['title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity']\n",
    "}\n",
    "\n",
    "# incrementally add feature groups and build models\n",
    "features = []\n",
    "results = []\n",
    "model_count = 0\n",
    "\n",
    "# loop through each feature group\n",
    "for group_name, group_features in feature_groups.items():\n",
    "    features.extend(group_features)\n",
    "    model_count += 1\n",
    "    model_name = f'Model Logistic A{model_count}'\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    model.fit(X_train[features], y_train)\n",
    "    preds_train = model.predict_proba(X_train[features])[:, 1]\n",
    "    preds_val = model.predict_proba(X_val[features])[:, 1]\n",
    "    auc_train = roc_auc_score(y_train, preds_train)\n",
    "    auc_val = roc_auc_score(y_val, preds_val)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'AUC Train': auc_train,\n",
    "        'AUC Validation': auc_val\n",
    "    })\n",
    "\n",
    "# convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# calcualte auc for a model with all features\n",
    "model_log_all = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "model_log_all.fit(X_train, y_train)\n",
    "preds_train = model_log_all.predict_proba(X_train)[:, 1]\n",
    "preds_val = model_log_all.predict_proba(X_val)[:, 1]\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Logistic A8',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show a consistent improvement in the AUC for both the training and validation datasets as more feature groups are incrementally added to the logistic regression models. Starting from Model Logistic A1 with basic article info, the AUC steadily increases, peaking at Model Logistic A8, which utilizes all available features, indicating that the additional features progressively enhance the model's ability to predict article popularity. The increase in AUC values from A1 to A8 highlights the importance of feature engineering in improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: Logistic Regression with LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  AUC Train  AUC Validation\n",
       "0    Model Logistic A1   0.592374        0.599277\n",
       "1    Model Logistic A2   0.615253        0.623459\n",
       "2    Model Logistic A3   0.681887        0.679263\n",
       "3    Model Logistic A4   0.685753        0.680492\n",
       "4    Model Logistic A5   0.686585        0.683885\n",
       "5    Model Logistic A6   0.690455        0.679908\n",
       "6    Model Logistic A7   0.691716        0.681179\n",
       "7    Model Logistic A8   0.695629        0.680625\n",
       "8   Model Log Lasso B1   0.592416        0.599329\n",
       "9   Model Log Lasso B2   0.607737        0.618729\n",
       "10  Model Log Lasso B3   0.680202        0.679111\n",
       "11  Model Log Lasso B4   0.684371        0.680592\n",
       "12  Model Log Lasso B5   0.684998        0.683703\n",
       "13  Model Log Lasso B6   0.690212        0.680255\n",
       "14  Model Log Lasso B7   0.691489        0.681548\n",
       "15  Model Log Lasso B8   0.695515        0.681085"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_lasso = []\n",
    "model_count = 0\n",
    "results_lasso = []\n",
    "\n",
    "# loop through each feature group\n",
    "for group_name, group_features_lasso in feature_groups.items():\n",
    "    features_lasso.extend(group_features_lasso)\n",
    "    model_count += 1\n",
    "    model_name = f'Model Log Lasso B{model_count}'\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(penalty='l1', C=1.0, solver='saga', max_iter=1000))\n",
    "    ])\n",
    "    model.fit(X_train[features_lasso], y_train)\n",
    "    preds_train = model.predict_proba(X_train[features_lasso])[:, 1]\n",
    "    preds_val = model.predict_proba(X_val[features_lasso])[:, 1]\n",
    "    auc_train = roc_auc_score(y_train, preds_train)\n",
    "    auc_val = roc_auc_score(y_val, preds_val)\n",
    "    results_lasso.append({\n",
    "        'Model': model_name,\n",
    "        'AUC Train': auc_train,\n",
    "        'AUC Validation': auc_val\n",
    "    })\n",
    "\n",
    "# convert results to DataFrame\n",
    "results_lasso_df = pd.DataFrame(results_lasso)\n",
    "\n",
    "# calculate AUC for a model with all features using Lasso\n",
    "model_log_lasso_all = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(penalty='l1', C=1.0, solver='saga', max_iter=1000))\n",
    "])\n",
    "model_log_lasso_all.fit(X_train, y_train)\n",
    "preds_train = model_log_lasso_all.predict_proba(X_train)[:, 1]\n",
    "preds_val = model_log_lasso_all.predict_proba(X_val)[:, 1]\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_lasso_df = pd.concat([results_lasso_df, pd.DataFrame([{\n",
    "    'Model': 'Model Log Lasso B8',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Combine the results\n",
    "results_df = pd.concat([results_df, results_lasso_df], ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across all Lasso models, the training and validation AUC scores are very close, suggesting that the models are well-calibrated and not overfitting significantly. The differences between the AUC scores on the training and validation sets are minimal, which is ideal in predictive modeling to ensure that the models generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters: {'rf__max_depth': 10, 'rf__min_samples_leaf': 4, 'rf__min_samples_split': 5, 'rf__n_estimators': 300}\n",
      "Validation AUC: 0.7035951151041098\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  AUC Train  AUC Validation\n",
       "0        Model Logistic A1   0.592374        0.599277\n",
       "1        Model Logistic A2   0.615253        0.623459\n",
       "2        Model Logistic A3   0.681887        0.679263\n",
       "3        Model Logistic A4   0.685753        0.680492\n",
       "4        Model Logistic A5   0.686585        0.683885\n",
       "5        Model Logistic A6   0.690455        0.679908\n",
       "6        Model Logistic A7   0.691716        0.681179\n",
       "7        Model Logistic A8   0.695629        0.680625\n",
       "8       Model Log Lasso B1   0.592416        0.599329\n",
       "9       Model Log Lasso B2   0.607737        0.618729\n",
       "10      Model Log Lasso B3   0.680202        0.679111\n",
       "11      Model Log Lasso B4   0.684371        0.680592\n",
       "12      Model Log Lasso B5   0.684998        0.683703\n",
       "13      Model Log Lasso B6   0.690212        0.680255\n",
       "14      Model Log Lasso B7   0.691489        0.681548\n",
       "15      Model Log Lasso B8   0.695515        0.681085\n",
       "16  Model Random Forest C1   0.712145        0.703595"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # we include it for consistency\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# parameters of Random Forest to tune\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 300],\n",
    "    'rf__max_depth': [10, 20],  \n",
    "    'rf__min_samples_split': [5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "# setup the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best model after grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# predictions and AUC score on validation data\n",
    "preds_val = best_rf.predict_proba(X_val)[:, 1]\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# output best parameters and validation AUC\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation AUC:\", auc_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Random Forest C1',\n",
    "    'AUC Train': grid_search.best_score_,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model (Model Random Forest C1) demonstrates superior performance compared to earlier logistic and Lasso models, achieving a notably higher AUC score on the training data and on the validation data. This indicates effective learning and generalization capabilities, highlighting Random Forest's robustness and its ability to handle the complexities and non-linear relationships within the dataset effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Best Parameters: {'gbm__learning_rate': 0.05, 'gbm__max_depth': 3, 'gbm__min_samples_leaf': 1, 'gbm__min_samples_split': 2, 'gbm__n_estimators': 200}\n",
      "Validation AUC: 0.7040332335316778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  AUC Train  AUC Validation\n",
       "0            Model Logistic A1   0.592374        0.599277\n",
       "1            Model Logistic A2   0.615253        0.623459\n",
       "2            Model Logistic A3   0.681887        0.679263\n",
       "3            Model Logistic A4   0.685753        0.680492\n",
       "4            Model Logistic A5   0.686585        0.683885\n",
       "5            Model Logistic A6   0.690455        0.679908\n",
       "6            Model Logistic A7   0.691716        0.681179\n",
       "7            Model Logistic A8   0.695629        0.680625\n",
       "8           Model Log Lasso B1   0.592416        0.599329\n",
       "9           Model Log Lasso B2   0.607737        0.618729\n",
       "10          Model Log Lasso B3   0.680202        0.679111\n",
       "11          Model Log Lasso B4   0.684371        0.680592\n",
       "12          Model Log Lasso B5   0.684998        0.683703\n",
       "13          Model Log Lasso B6   0.690212        0.680255\n",
       "14          Model Log Lasso B7   0.691489        0.681548\n",
       "15          Model Log Lasso B8   0.695515        0.681085\n",
       "16      Model Random Forest C1   0.712145        0.703595\n",
       "17  Model Gradient Boosting D1   0.711294        0.704033"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbm\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Not needed for GBM but keeping for consistency\n",
    "    ('gbm', GradientBoostingClassifier(random_state=prng))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'gbm__n_estimators': [100, 200],\n",
    "    'gbm__learning_rate': [0.05],\n",
    "    'gbm__max_depth': [3, 5],\n",
    "    'gbm__min_samples_split': [2], \n",
    "    'gbm__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# setup the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=4, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best model after grid search\n",
    "best_gbm = grid_search.best_estimator_\n",
    "\n",
    "# predictions and AUC score on validation data\n",
    "preds_val = best_gbm.predict_proba(X_val)[:, 1]\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# output best parameters and validation AUC\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation AUC:\", auc_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Gradient Boosting D1',\n",
    "    'AUC Train': grid_search.best_score_,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting machine showed similar results to Random Forest.\n",
    "This performance indicates that the model is well-tuned, balancing bias and variance effectively to achieve strong predictive accuracy without significant overfitting. The results highlight the GBM's capability to capture complex non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8703 - loss: 0.3945 - val_accuracy: 0.8764 - val_loss: 0.3547\n",
      "Epoch 2/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8758 - loss: 0.3581 - val_accuracy: 0.8766 - val_loss: 0.3579\n",
      "Epoch 3/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8775 - loss: 0.3507 - val_accuracy: 0.8771 - val_loss: 0.3500\n",
      "Epoch 4/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8754 - loss: 0.3494 - val_accuracy: 0.8771 - val_loss: 0.3527\n",
      "Epoch 5/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8808 - loss: 0.3370 - val_accuracy: 0.8771 - val_loss: 0.3516\n",
      "Epoch 6/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8767 - loss: 0.3433 - val_accuracy: 0.8772 - val_loss: 0.3492\n",
      "Epoch 7/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.3406 - val_accuracy: 0.8769 - val_loss: 0.3501\n",
      "Epoch 8/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.3376 - val_accuracy: 0.8771 - val_loss: 0.3523\n",
      "Epoch 9/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8789 - loss: 0.3388 - val_accuracy: 0.8774 - val_loss: 0.3490\n",
      "Epoch 10/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8804 - loss: 0.3300 - val_accuracy: 0.8772 - val_loss: 0.3509\n",
      "Epoch 11/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8799 - loss: 0.3317 - val_accuracy: 0.8771 - val_loss: 0.3497\n",
      "Epoch 12/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8786 - loss: 0.3345 - val_accuracy: 0.8771 - val_loss: 0.3507\n",
      "Epoch 13/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8759 - loss: 0.3380 - val_accuracy: 0.8766 - val_loss: 0.3498\n",
      "Epoch 14/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8803 - loss: 0.3298 - val_accuracy: 0.8772 - val_loss: 0.3489\n",
      "Epoch 15/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.3273 - val_accuracy: 0.8769 - val_loss: 0.3497\n",
      "Epoch 16/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8802 - loss: 0.3264 - val_accuracy: 0.8774 - val_loss: 0.3487\n",
      "Epoch 17/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8796 - loss: 0.3299 - val_accuracy: 0.8767 - val_loss: 0.3504\n",
      "Epoch 18/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8805 - loss: 0.3254 - val_accuracy: 0.8771 - val_loss: 0.3501\n",
      "Epoch 19/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.3328 - val_accuracy: 0.8769 - val_loss: 0.3517\n",
      "Epoch 20/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8771 - loss: 0.3335 - val_accuracy: 0.8756 - val_loss: 0.3512\n",
      "Epoch 21/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8800 - loss: 0.3230 - val_accuracy: 0.8769 - val_loss: 0.3530\n",
      "Epoch 22/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8807 - loss: 0.3214 - val_accuracy: 0.8766 - val_loss: 0.3519\n",
      "Epoch 23/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8807 - loss: 0.3233 - val_accuracy: 0.8771 - val_loss: 0.3520\n",
      "Epoch 24/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8786 - loss: 0.3276 - val_accuracy: 0.8764 - val_loss: 0.3550\n",
      "Epoch 25/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8802 - loss: 0.3227 - val_accuracy: 0.8776 - val_loss: 0.3523\n",
      "Epoch 26/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8789 - loss: 0.3259 - val_accuracy: 0.8762 - val_loss: 0.3504\n",
      "Epoch 27/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8793 - loss: 0.3257 - val_accuracy: 0.8769 - val_loss: 0.3537\n",
      "Epoch 28/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8825 - loss: 0.3185 - val_accuracy: 0.8769 - val_loss: 0.3561\n",
      "Epoch 29/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8820 - loss: 0.3184 - val_accuracy: 0.8771 - val_loss: 0.3516\n",
      "Epoch 30/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8829 - loss: 0.3152 - val_accuracy: 0.8747 - val_loss: 0.3553\n",
      "Epoch 31/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8787 - loss: 0.3216 - val_accuracy: 0.8766 - val_loss: 0.3567\n",
      "Epoch 32/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8822 - loss: 0.3171 - val_accuracy: 0.8752 - val_loss: 0.3568\n",
      "Epoch 33/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8843 - loss: 0.3115 - val_accuracy: 0.8762 - val_loss: 0.3557\n",
      "Epoch 34/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8785 - loss: 0.3213 - val_accuracy: 0.8764 - val_loss: 0.3530\n",
      "Epoch 35/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8816 - loss: 0.3194 - val_accuracy: 0.8754 - val_loss: 0.3565\n",
      "Epoch 36/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8848 - loss: 0.3102 - val_accuracy: 0.8751 - val_loss: 0.3610\n",
      "Epoch 37/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 0.3142 - val_accuracy: 0.8757 - val_loss: 0.3574\n",
      "Epoch 38/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8812 - loss: 0.3175 - val_accuracy: 0.8741 - val_loss: 0.3573\n",
      "Epoch 39/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8826 - loss: 0.3109 - val_accuracy: 0.8747 - val_loss: 0.3590\n",
      "Epoch 40/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8833 - loss: 0.3174 - val_accuracy: 0.8756 - val_loss: 0.3598\n",
      "Epoch 41/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.3127 - val_accuracy: 0.8742 - val_loss: 0.3568\n",
      "Epoch 42/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8861 - loss: 0.3051 - val_accuracy: 0.8737 - val_loss: 0.3576\n",
      "Epoch 43/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8844 - loss: 0.3094 - val_accuracy: 0.8744 - val_loss: 0.3604\n",
      "Epoch 44/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.3151 - val_accuracy: 0.8741 - val_loss: 0.3563\n",
      "Epoch 45/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8818 - loss: 0.3119 - val_accuracy: 0.8751 - val_loss: 0.3623\n",
      "Epoch 46/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8846 - loss: 0.3101 - val_accuracy: 0.8734 - val_loss: 0.3573\n",
      "Epoch 47/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8825 - loss: 0.3137 - val_accuracy: 0.8742 - val_loss: 0.3613\n",
      "Epoch 48/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.3120 - val_accuracy: 0.8752 - val_loss: 0.3588\n",
      "Epoch 49/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8885 - loss: 0.2972 - val_accuracy: 0.8746 - val_loss: 0.3614\n",
      "Epoch 50/50\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8821 - loss: 0.3078 - val_accuracy: 0.8746 - val_loss: 0.3591\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778us/step\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step\n",
      "Training AUC: 0.8497724951672319\n",
      "Validation AUC: 0.6752898584173289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  AUC Train  AUC Validation\n",
       "0            Model Logistic A1   0.592374        0.599277\n",
       "1            Model Logistic A2   0.615253        0.623459\n",
       "2            Model Logistic A3   0.681887        0.679263\n",
       "3            Model Logistic A4   0.685753        0.680492\n",
       "4            Model Logistic A5   0.686585        0.683885\n",
       "5            Model Logistic A6   0.690455        0.679908\n",
       "6            Model Logistic A7   0.691716        0.681179\n",
       "7            Model Logistic A8   0.695629        0.680625\n",
       "8           Model Log Lasso B1   0.592416        0.599329\n",
       "9           Model Log Lasso B2   0.607737        0.618729\n",
       "10          Model Log Lasso B3   0.680202        0.679111\n",
       "11          Model Log Lasso B4   0.684371        0.680592\n",
       "12          Model Log Lasso B5   0.684998        0.683703\n",
       "13          Model Log Lasso B6   0.690212        0.680255\n",
       "14          Model Log Lasso B7   0.691489        0.681548\n",
       "15          Model Log Lasso B8   0.695515        0.681085\n",
       "16      Model Random Forest C1   0.712145        0.703595\n",
       "17  Model Gradient Boosting D1   0.711294        0.704033\n",
       "18     Model Neural Network E1   0.849772        0.675290"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))  # Input layer and first hidden layer with ReLU activation\n",
    "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
    "model.add(Dense(64, activation='relu'))  # Second hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict probabilities for the training and validation set\n",
    "preds_train = model.predict(X_train_scaled)\n",
    "preds_val = model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the AUC score for training and validation\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f\"Training AUC: {auc_train}\")\n",
    "print(f\"Validation AUC: {auc_val}\")\n",
    "\n",
    "# Add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Neural Network E1',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network (Model Neural Network E1) achieved an impressive training AUC of 0.852537, suggesting that it fits the training data well and captures complex patterns effectively. However, the validation AUC of 0.674486 indicates a significant drop in performance on unseen data, suggesting potential overfitting. This discrepancy highlights the need for further tuning of the model's parameters or architecture to enhance its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8721 - loss: 0.3934 - val_accuracy: 0.8769 - val_loss: 0.3537\n",
      "Epoch 2/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8765 - loss: 0.3614 - val_accuracy: 0.8766 - val_loss: 0.3522\n",
      "Epoch 3/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 0.3468 - val_accuracy: 0.8771 - val_loss: 0.3533\n",
      "Epoch 4/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8788 - loss: 0.3479 - val_accuracy: 0.8769 - val_loss: 0.3543\n",
      "Epoch 5/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8814 - loss: 0.3431 - val_accuracy: 0.8771 - val_loss: 0.3484\n",
      "Epoch 6/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8797 - loss: 0.3419 - val_accuracy: 0.8771 - val_loss: 0.3507\n",
      "Epoch 7/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8764 - loss: 0.3441 - val_accuracy: 0.8767 - val_loss: 0.3487\n",
      "Epoch 8/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8808 - loss: 0.3358 - val_accuracy: 0.8769 - val_loss: 0.3547\n",
      "Epoch 9/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8788 - loss: 0.3376 - val_accuracy: 0.8771 - val_loss: 0.3505\n",
      "Epoch 10/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8786 - loss: 0.3384 - val_accuracy: 0.8771 - val_loss: 0.3458\n",
      "Epoch 11/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8755 - loss: 0.3450 - val_accuracy: 0.8771 - val_loss: 0.3493\n",
      "Epoch 12/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.3463 - val_accuracy: 0.8771 - val_loss: 0.3472\n",
      "Epoch 13/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8767 - loss: 0.3401 - val_accuracy: 0.8771 - val_loss: 0.3468\n",
      "Epoch 14/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8809 - loss: 0.3317 - val_accuracy: 0.8769 - val_loss: 0.3476\n",
      "Epoch 15/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.3347 - val_accuracy: 0.8771 - val_loss: 0.3473\n",
      "Epoch 16/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.3372 - val_accuracy: 0.8771 - val_loss: 0.3477\n",
      "Epoch 17/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8814 - loss: 0.3297 - val_accuracy: 0.8771 - val_loss: 0.3491\n",
      "Epoch 18/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8747 - loss: 0.3389 - val_accuracy: 0.8769 - val_loss: 0.3520\n",
      "Epoch 19/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8768 - loss: 0.3373 - val_accuracy: 0.8767 - val_loss: 0.3499\n",
      "Epoch 20/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8775 - loss: 0.3349 - val_accuracy: 0.8769 - val_loss: 0.3503\n",
      "Epoch 21/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8828 - loss: 0.3252 - val_accuracy: 0.8772 - val_loss: 0.3549\n",
      "Epoch 22/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8782 - loss: 0.3345 - val_accuracy: 0.8771 - val_loss: 0.3516\n",
      "Epoch 23/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8777 - loss: 0.3334 - val_accuracy: 0.8771 - val_loss: 0.3537\n",
      "Epoch 24/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8788 - loss: 0.3325 - val_accuracy: 0.8767 - val_loss: 0.3514\n",
      "Epoch 25/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 0.3228 - val_accuracy: 0.8764 - val_loss: 0.3582\n",
      "Epoch 26/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8813 - loss: 0.3277 - val_accuracy: 0.8769 - val_loss: 0.3540\n",
      "Epoch 27/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.3270 - val_accuracy: 0.8769 - val_loss: 0.3576\n",
      "Epoch 28/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8799 - loss: 0.3263 - val_accuracy: 0.8772 - val_loss: 0.3499\n",
      "Epoch 29/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8811 - loss: 0.3250 - val_accuracy: 0.8772 - val_loss: 0.3551\n",
      "Epoch 30/30\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8785 - loss: 0.3287 - val_accuracy: 0.8769 - val_loss: 0.3556\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step\n",
      "Training AUC: 0.7811200582253213\n",
      "Validation AUC: 0.6928486903393116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  AUC Train  AUC Validation\n",
       "0            Model Logistic A1   0.592374        0.599277\n",
       "1            Model Logistic A2   0.615253        0.623459\n",
       "2            Model Logistic A3   0.681887        0.679263\n",
       "3            Model Logistic A4   0.685753        0.680492\n",
       "4            Model Logistic A5   0.686585        0.683885\n",
       "5            Model Logistic A6   0.690455        0.679908\n",
       "6            Model Logistic A7   0.691716        0.681179\n",
       "7            Model Logistic A8   0.695629        0.680625\n",
       "8           Model Log Lasso B1   0.592416        0.599329\n",
       "9           Model Log Lasso B2   0.607737        0.618729\n",
       "10          Model Log Lasso B3   0.680202        0.679111\n",
       "11          Model Log Lasso B4   0.684371        0.680592\n",
       "12          Model Log Lasso B5   0.684998        0.683703\n",
       "13          Model Log Lasso B6   0.690212        0.680255\n",
       "14          Model Log Lasso B7   0.691489        0.681548\n",
       "15          Model Log Lasso B8   0.695515        0.681085\n",
       "16      Model Random Forest C1   0.712145        0.703595\n",
       "17  Model Gradient Boosting D1   0.711294        0.704033\n",
       "18     Model Neural Network E1   0.849772        0.675290\n",
       "19     Model Neural Network E2   0.781120        0.692849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))  # Input layer and first hidden layer with ReLU activation\n",
    "model2.add(Dropout(0.6))  # Increased dropout for regularization\n",
    "model2.add(Dense(64, activation='relu'))  # Second hidden layer\n",
    "model2.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "# Compile the model2 with an adjusted learning rate\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model2 on the training data\n",
    "history = model2.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=30, batch_size=64, verbose=1)  # Adjusted epochs and batch size\n",
    "\n",
    "# Predict probabilities for the training and validation set\n",
    "preds_train = model2.predict(X_train_scaled)\n",
    "preds_val = model2.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the AUC score for training and validation\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f\"Training AUC: {auc_train}\")\n",
    "print(f\"Validation AUC: {auc_val}\")\n",
    "\n",
    "# Add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Neural Network E2',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted Neural Network model (Model Neural Network E2) shows improved generalization compared to its predecessor (Model E1), with a validation AUC of 0.695402, up from 0.674486, indicating a reduction in overfitting as reflected by a closer alignment of training and validation scores. The training AUC of 0.774155, although lower than E1's 0.852537, suggests that the model is now less overfitted to the training data, making it a more reliable predictor for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7919 - loss: 0.5619 - val_accuracy: 0.8762 - val_loss: 0.3705\n",
      "Epoch 2/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8705 - loss: 0.4057 - val_accuracy: 0.8762 - val_loss: 0.3696\n",
      "Epoch 3/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8781 - loss: 0.3749 - val_accuracy: 0.8771 - val_loss: 0.3648\n",
      "Epoch 4/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.3656 - val_accuracy: 0.8771 - val_loss: 0.3652\n",
      "Epoch 5/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.3665 - val_accuracy: 0.8771 - val_loss: 0.3594\n",
      "Epoch 6/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.3615 - val_accuracy: 0.8771 - val_loss: 0.3576\n",
      "Epoch 7/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8788 - loss: 0.3509 - val_accuracy: 0.8771 - val_loss: 0.3538\n",
      "Epoch 8/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8766 - loss: 0.3548 - val_accuracy: 0.8771 - val_loss: 0.3539\n",
      "Epoch 9/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3517 - val_accuracy: 0.8771 - val_loss: 0.3536\n",
      "Epoch 10/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3508 - val_accuracy: 0.8771 - val_loss: 0.3547\n",
      "Epoch 11/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8794 - loss: 0.3454 - val_accuracy: 0.8771 - val_loss: 0.3511\n",
      "Epoch 12/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8797 - loss: 0.3442 - val_accuracy: 0.8771 - val_loss: 0.3498\n",
      "Epoch 13/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3564 - val_accuracy: 0.8771 - val_loss: 0.3491\n",
      "Epoch 14/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8795 - loss: 0.3393 - val_accuracy: 0.8771 - val_loss: 0.3508\n",
      "Epoch 15/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3427 - val_accuracy: 0.8771 - val_loss: 0.3527\n",
      "Epoch 16/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.3371 - val_accuracy: 0.8771 - val_loss: 0.3492\n",
      "Epoch 17/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8828 - loss: 0.3326 - val_accuracy: 0.8771 - val_loss: 0.3500\n",
      "Epoch 18/100\n",
      "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 0.3412 - val_accuracy: 0.8771 - val_loss: 0.3509\n",
      "\u001b[1m744/744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 807us/step\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step\n",
      "Training AUC: 0.731519105598053\n",
      "Validation AUC: 0.6946675178132318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  AUC Train  AUC Validation\n",
       "0            Model Logistic A1   0.592374        0.599277\n",
       "1            Model Logistic A2   0.615253        0.623459\n",
       "2            Model Logistic A3   0.681887        0.679263\n",
       "3            Model Logistic A4   0.685753        0.680492\n",
       "4            Model Logistic A5   0.686585        0.683885\n",
       "5            Model Logistic A6   0.690455        0.679908\n",
       "6            Model Logistic A7   0.691716        0.681179\n",
       "7            Model Logistic A8   0.695629        0.680625\n",
       "8           Model Log Lasso B1   0.592416        0.599329\n",
       "9           Model Log Lasso B2   0.607737        0.618729\n",
       "10          Model Log Lasso B3   0.680202        0.679111\n",
       "11          Model Log Lasso B4   0.684371        0.680592\n",
       "12          Model Log Lasso B5   0.684998        0.683703\n",
       "13          Model Log Lasso B6   0.690212        0.680255\n",
       "14          Model Log Lasso B7   0.691489        0.681548\n",
       "15          Model Log Lasso B8   0.695515        0.681085\n",
       "16      Model Random Forest C1   0.712145        0.703595\n",
       "17  Model Gradient Boosting D1   0.711294        0.704033\n",
       "18     Model Neural Network E1   0.849772        0.675290\n",
       "19     Model Neural Network E2   0.781120        0.692849\n",
       "20     Model Neural Network E3   0.731519        0.694668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(100, input_dim=X_train_scaled.shape[1], activation='relu'))  # Adjusted number of neurons\n",
    "model3.add(BatchNormalization())  # Adding batch normalization\n",
    "model3.add(Dropout(0.6))  # Increased dropout for regularization\n",
    "model3.add(Dense(50, activation='relu'))  # Adjusted number of neurons\n",
    "model3.add(Dropout(0.5))  # Adding another dropout layer\n",
    "model3.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "# Compile the model3 with an adjusted learning rate\n",
    "model3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model3 on the training data\n",
    "history = model3.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), \n",
    "                    epochs=100, batch_size=64, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Predict probabilities for the training and validation set\n",
    "preds_train = model3.predict(X_train_scaled)\n",
    "preds_val = model3.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the AUC score for training and validation\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f\"Training AUC: {auc_train}\")\n",
    "print(f\"Validation AUC: {auc_val}\")\n",
    "\n",
    "# Add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Neural Network E3',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E3 demonstrates similar results to E2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model F: EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.768599690968112\n",
      "Validation AUC: 0.7023037082574506\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Explainable Boosting F1</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.702304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  AUC Train  AUC Validation\n",
       "0               Model Logistic A1   0.592374        0.599277\n",
       "1               Model Logistic A2   0.615253        0.623459\n",
       "2               Model Logistic A3   0.681887        0.679263\n",
       "3               Model Logistic A4   0.685753        0.680492\n",
       "4               Model Logistic A5   0.686585        0.683885\n",
       "5               Model Logistic A6   0.690455        0.679908\n",
       "6               Model Logistic A7   0.691716        0.681179\n",
       "7               Model Logistic A8   0.695629        0.680625\n",
       "8              Model Log Lasso B1   0.592416        0.599329\n",
       "9              Model Log Lasso B2   0.607737        0.618729\n",
       "10             Model Log Lasso B3   0.680202        0.679111\n",
       "11             Model Log Lasso B4   0.684371        0.680592\n",
       "12             Model Log Lasso B5   0.684998        0.683703\n",
       "13             Model Log Lasso B6   0.690212        0.680255\n",
       "14             Model Log Lasso B7   0.691489        0.681548\n",
       "15             Model Log Lasso B8   0.695515        0.681085\n",
       "16         Model Random Forest C1   0.712145        0.703595\n",
       "17     Model Gradient Boosting D1   0.711294        0.704033\n",
       "18        Model Neural Network E1   0.849772        0.675290\n",
       "19        Model Neural Network E2   0.781120        0.692849\n",
       "20        Model Neural Network E3   0.731519        0.694668\n",
       "21  Model Explainable Boosting F1   0.768600        0.702304"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EBM\n",
    "ebm = ExplainableBoostingClassifier(random_state=20240418)\n",
    "ebm.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the training and validation set\n",
    "preds_train = ebm.predict_proba(X_train)[:, 1]\n",
    "preds_val = ebm.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for training and validation\n",
    "auc_train = roc_auc_score(y_train, preds_train)\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f\"Training AUC: {auc_train}\")\n",
    "print(f\"Validation AUC: {auc_val}\")\n",
    "\n",
    "# Store results in a DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Explainable Boosting F1',\n",
    "    'AUC Train': auc_train,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Print the updated results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Explainable Boosting Machine (Model Explainable Boosting F1) achieves a robust training AUC and an equally impressive validation AUC, indicating that it not only fits the training data well but also generalizes effectively to unseen data. This performance places it competitively among the top models, combining high interpretability with strong predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC (EBM2): 0.7380603772709037\n",
      "Validation AUC (EBM2): 0.7040485762003474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Explainable Boosting F1</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.702304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Explainable Boosting F2</td>\n",
       "      <td>0.738060</td>\n",
       "      <td>0.704049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  AUC Train  AUC Validation\n",
       "0               Model Logistic A1   0.592374        0.599277\n",
       "1               Model Logistic A2   0.615253        0.623459\n",
       "2               Model Logistic A3   0.681887        0.679263\n",
       "3               Model Logistic A4   0.685753        0.680492\n",
       "4               Model Logistic A5   0.686585        0.683885\n",
       "5               Model Logistic A6   0.690455        0.679908\n",
       "6               Model Logistic A7   0.691716        0.681179\n",
       "7               Model Logistic A8   0.695629        0.680625\n",
       "8              Model Log Lasso B1   0.592416        0.599329\n",
       "9              Model Log Lasso B2   0.607737        0.618729\n",
       "10             Model Log Lasso B3   0.680202        0.679111\n",
       "11             Model Log Lasso B4   0.684371        0.680592\n",
       "12             Model Log Lasso B5   0.684998        0.683703\n",
       "13             Model Log Lasso B6   0.690212        0.680255\n",
       "14             Model Log Lasso B7   0.691489        0.681548\n",
       "15             Model Log Lasso B8   0.695515        0.681085\n",
       "16         Model Random Forest C1   0.712145        0.703595\n",
       "17     Model Gradient Boosting D1   0.711294        0.704033\n",
       "18        Model Neural Network E1   0.849772        0.675290\n",
       "19        Model Neural Network E2   0.781120        0.692849\n",
       "20        Model Neural Network E3   0.731519        0.694668\n",
       "21  Model Explainable Boosting F1   0.768600        0.702304\n",
       "22  Model Explainable Boosting F2   0.738060        0.704049"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ebm2 = ExplainableBoostingClassifier(random_state=20240418, learning_rate=0.01, max_bins=256, max_interaction_bins=32, interactions=2)\n",
    "ebm2.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities for the training and validation set\n",
    "preds_train_ebm2 = ebm2.predict_proba(X_train)[:, 1]\n",
    "preds_val_ebm2 = ebm2.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# calculate the AUC score for training and validation\n",
    "auc_train_ebm2 = roc_auc_score(y_train, preds_train_ebm2)\n",
    "auc_val_ebm2 = roc_auc_score(y_val, preds_val_ebm2)\n",
    "\n",
    "# print the AUC score\n",
    "print(f\"Training AUC (EBM2): {auc_train_ebm2}\")\n",
    "print(f\"Validation AUC (EBM2): {auc_val_ebm2}\")\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model Explainable Boosting F2',\n",
    "    'AUC Train': auc_train_ebm2,\n",
    "    'AUC Validation': auc_val_ebm2\n",
    "}])], ignore_index=True)\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Explainable Boosting F2 shows a decrease in training AUC to 0.741042 from F1's 0.771353, indicating a slight reduction in how well the model fits the training data, potentially due to the modifications aimed at enhancing generalization. However, these adjustments yield a slight improvement in validation AUC to 0.704875 from 0.704272, suggesting that F2 generalizes marginally better to unseen data compared to F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model G: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Best Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation AUC: 0.6068720468641159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Explainable Boosting F1</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.702304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Explainable Boosting F2</td>\n",
       "      <td>0.738060</td>\n",
       "      <td>0.704049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Model SVM G</td>\n",
       "      <td>0.625799</td>\n",
       "      <td>0.606872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  AUC Train  AUC Validation\n",
       "0               Model Logistic A1   0.592374        0.599277\n",
       "1               Model Logistic A2   0.615253        0.623459\n",
       "2               Model Logistic A3   0.681887        0.679263\n",
       "3               Model Logistic A4   0.685753        0.680492\n",
       "4               Model Logistic A5   0.686585        0.683885\n",
       "5               Model Logistic A6   0.690455        0.679908\n",
       "6               Model Logistic A7   0.691716        0.681179\n",
       "7               Model Logistic A8   0.695629        0.680625\n",
       "8              Model Log Lasso B1   0.592416        0.599329\n",
       "9              Model Log Lasso B2   0.607737        0.618729\n",
       "10             Model Log Lasso B3   0.680202        0.679111\n",
       "11             Model Log Lasso B4   0.684371        0.680592\n",
       "12             Model Log Lasso B5   0.684998        0.683703\n",
       "13             Model Log Lasso B6   0.690212        0.680255\n",
       "14             Model Log Lasso B7   0.691489        0.681548\n",
       "15             Model Log Lasso B8   0.695515        0.681085\n",
       "16         Model Random Forest C1   0.712145        0.703595\n",
       "17     Model Gradient Boosting D1   0.711294        0.704033\n",
       "18        Model Neural Network E1   0.849772        0.675290\n",
       "19        Model Neural Network E2   0.781120        0.692849\n",
       "20        Model Neural Network E3   0.731519        0.694668\n",
       "21  Model Explainable Boosting F1   0.768600        0.702304\n",
       "22  Model Explainable Boosting F2   0.738060        0.704049\n",
       "23                    Model SVM G   0.625799        0.606872"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# define the SVM pipeline and grid search parameters\n",
    "pipeline = Pipeline([\n",
    "    ('svm', SVC(probability=True, random_state=20240418))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C': [1, 10], \n",
    "    'svm__gamma': ['scale'],  \n",
    "    'svm__kernel': ['rbf']  \n",
    "}\n",
    "\n",
    "# setup the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='roc_auc', verbose=1, n_jobs=-1)  # Reduced the number of folds\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# best model after grid search\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# predict probabilities for the validation set\n",
    "preds_val = best_svm.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# calculate the AUC score for the validation\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# output best parameters and validation AUC\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation AUC:\", auc_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model SVM G',\n",
    "    'AUC Train': grid_search.best_score_,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model (Model SVM G) achieved a training AUC of 0.625799 and a validation AUC of 0.606872, indicating a moderate level of performance that suggests the model could benefit from further parameter tuning or exploration of more complex models to better capture the underlying patterns in the data.\n",
    "We are not going to improve this model as it takes a lot of computation power and is very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model H: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Validation AUC: 0.7048985862714324\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Explainable Boosting F1</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.702304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Explainable Boosting F2</td>\n",
       "      <td>0.738060</td>\n",
       "      <td>0.704049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Model SVM G</td>\n",
       "      <td>0.625799</td>\n",
       "      <td>0.606872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Model XGBoost H1</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.704899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  AUC Train  AUC Validation\n",
       "0               Model Logistic A1   0.592374        0.599277\n",
       "1               Model Logistic A2   0.615253        0.623459\n",
       "2               Model Logistic A3   0.681887        0.679263\n",
       "3               Model Logistic A4   0.685753        0.680492\n",
       "4               Model Logistic A5   0.686585        0.683885\n",
       "5               Model Logistic A6   0.690455        0.679908\n",
       "6               Model Logistic A7   0.691716        0.681179\n",
       "7               Model Logistic A8   0.695629        0.680625\n",
       "8              Model Log Lasso B1   0.592416        0.599329\n",
       "9              Model Log Lasso B2   0.607737        0.618729\n",
       "10             Model Log Lasso B3   0.680202        0.679111\n",
       "11             Model Log Lasso B4   0.684371        0.680592\n",
       "12             Model Log Lasso B5   0.684998        0.683703\n",
       "13             Model Log Lasso B6   0.690212        0.680255\n",
       "14             Model Log Lasso B7   0.691489        0.681548\n",
       "15             Model Log Lasso B8   0.695515        0.681085\n",
       "16         Model Random Forest C1   0.712145        0.703595\n",
       "17     Model Gradient Boosting D1   0.711294        0.704033\n",
       "18        Model Neural Network E1   0.849772        0.675290\n",
       "19        Model Neural Network E2   0.781120        0.692849\n",
       "20        Model Neural Network E3   0.731519        0.694668\n",
       "21  Model Explainable Boosting F1   0.768600        0.702304\n",
       "22  Model Explainable Boosting F2   0.738060        0.704049\n",
       "23                    Model SVM G   0.625799        0.606872\n",
       "24               Model XGBoost H1   0.716165        0.704899"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=20240418)\n",
    "\n",
    "# parameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best model after grid search\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# predict probabilities for the validation set\n",
    "preds_val = best_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# calculate the AUC score for the validation\n",
    "auc_val = roc_auc_score(y_val, preds_val)\n",
    "\n",
    "# output best parameters and validation AUC\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation AUC:\", auc_val)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model XGBoost H1',\n",
    "    'AUC Train': grid_search.best_score_,\n",
    "    'AUC Validation': auc_val\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost performed the best so far. We will try to further improve it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'colsample_bytree': 0.75, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.75}\n",
      "Validation AUC: 0.7089637377993001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Logistic A1</td>\n",
       "      <td>0.592374</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Logistic A2</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.623459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Logistic A3</td>\n",
       "      <td>0.681887</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Logistic A4</td>\n",
       "      <td>0.685753</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Logistic A5</td>\n",
       "      <td>0.686585</td>\n",
       "      <td>0.683885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Logistic A6</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>0.679908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Logistic A7</td>\n",
       "      <td>0.691716</td>\n",
       "      <td>0.681179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model Logistic A8</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Log Lasso B1</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.599329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Log Lasso B2</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>0.618729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model Log Lasso B3</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model Log Lasso B4</td>\n",
       "      <td>0.684371</td>\n",
       "      <td>0.680592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model Log Lasso B5</td>\n",
       "      <td>0.684998</td>\n",
       "      <td>0.683703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model Log Lasso B6</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model Log Lasso B7</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.681548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Model Log Lasso B8</td>\n",
       "      <td>0.695515</td>\n",
       "      <td>0.681085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Model Random Forest C1</td>\n",
       "      <td>0.712145</td>\n",
       "      <td>0.703595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Model Gradient Boosting D1</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.704033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Model Neural Network E1</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.675290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Model Neural Network E2</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.692849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Model Neural Network E3</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>0.694668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Model Explainable Boosting F1</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.702304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Explainable Boosting F2</td>\n",
       "      <td>0.738060</td>\n",
       "      <td>0.704049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Model SVM G</td>\n",
       "      <td>0.625799</td>\n",
       "      <td>0.606872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Model XGBoost H1</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.704899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Model XGBoost H2</td>\n",
       "      <td>0.718639</td>\n",
       "      <td>0.708964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  AUC Train  AUC Validation\n",
       "0               Model Logistic A1   0.592374        0.599277\n",
       "1               Model Logistic A2   0.615253        0.623459\n",
       "2               Model Logistic A3   0.681887        0.679263\n",
       "3               Model Logistic A4   0.685753        0.680492\n",
       "4               Model Logistic A5   0.686585        0.683885\n",
       "5               Model Logistic A6   0.690455        0.679908\n",
       "6               Model Logistic A7   0.691716        0.681179\n",
       "7               Model Logistic A8   0.695629        0.680625\n",
       "8              Model Log Lasso B1   0.592416        0.599329\n",
       "9              Model Log Lasso B2   0.607737        0.618729\n",
       "10             Model Log Lasso B3   0.680202        0.679111\n",
       "11             Model Log Lasso B4   0.684371        0.680592\n",
       "12             Model Log Lasso B5   0.684998        0.683703\n",
       "13             Model Log Lasso B6   0.690212        0.680255\n",
       "14             Model Log Lasso B7   0.691489        0.681548\n",
       "15             Model Log Lasso B8   0.695515        0.681085\n",
       "16         Model Random Forest C1   0.712145        0.703595\n",
       "17     Model Gradient Boosting D1   0.711294        0.704033\n",
       "18        Model Neural Network E1   0.849772        0.675290\n",
       "19        Model Neural Network E2   0.781120        0.692849\n",
       "20        Model Neural Network E3   0.731519        0.694668\n",
       "21  Model Explainable Boosting F1   0.768600        0.702304\n",
       "22  Model Explainable Boosting F2   0.738060        0.704049\n",
       "23                    Model SVM G   0.625799        0.606872\n",
       "24               Model XGBoost H1   0.716165        0.704899\n",
       "25               Model XGBoost H2   0.718639        0.708964"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=20240418)\n",
    "\n",
    "# expanded Parameters for GridSearchCV\n",
    "param_grid_h2 = {\n",
    "    'max_depth': [3, 6], \n",
    "    'n_estimators': [100, 200],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'subsample': [0.75, 1.0], \n",
    "    'colsample_bytree': [0.75, 1.0]\n",
    "}\n",
    "\n",
    "# setup the GridSearchCV object\n",
    "grid_search_2 = GridSearchCV(xgb_model, param_grid_h2, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit XGBoost model\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "\n",
    "# best model after grid search\n",
    "best_xgb_2 = grid_search_2.best_estimator_\n",
    "\n",
    "# predict probabilities for the validation set\n",
    "preds_val_gmb_2 = best_xgb_2.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# calculate the AUC score for the validation\n",
    "auc_val_gbm2 = roc_auc_score(y_val, preds_val_gmb_2)\n",
    "\n",
    "# output best parameters and validation AUC\n",
    "print(\"Best Parameters:\", grid_search_2.best_params_)\n",
    "print(\"Validation AUC:\", auc_val_gbm2)\n",
    "\n",
    "# add the results to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "    'Model': 'Model XGBoost H2',\n",
    "    'AUC Train': grid_search_2.best_score_,\n",
    "    'AUC Validation': auc_val_gbm2\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model H2 stands out as the most effective model so far, demonstrating the highest validation performance among all models tested. This indicates its superior ability to generalize well to unseen data while maintaining a strong balance between complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>11</td>\n",
       "      <td>217</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.488479</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>591.0</td>\n",
       "      <td>174.6</td>\n",
       "      <td>16100</td>\n",
       "      <td>843300</td>\n",
       "      <td>334080.000000</td>\n",
       "      <td>2918.939394</td>\n",
       "      <td>3700.494673</td>\n",
       "      <td>3248.890010</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>16100</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040064</td>\n",
       "      <td>0.839932</td>\n",
       "      <td>0.380880</td>\n",
       "      <td>0.094661</td>\n",
       "      <td>0.023041</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.367273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.170370</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415</td>\n",
       "      <td>11</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.489423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700321</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4.716619</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>417.0</td>\n",
       "      <td>116.4</td>\n",
       "      <td>68300</td>\n",
       "      <td>843300</td>\n",
       "      <td>688300.000000</td>\n",
       "      <td>3396.488751</td>\n",
       "      <td>6678.414343</td>\n",
       "      <td>5181.926731</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1900</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040021</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.839832</td>\n",
       "      <td>0.040147</td>\n",
       "      <td>0.564888</td>\n",
       "      <td>0.174474</td>\n",
       "      <td>0.066282</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.452530</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.426268</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625</td>\n",
       "      <td>9</td>\n",
       "      <td>486</td>\n",
       "      <td>0.599585</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.862140</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>343.0</td>\n",
       "      <td>247.5</td>\n",
       "      <td>36200</td>\n",
       "      <td>690400</td>\n",
       "      <td>349400.000000</td>\n",
       "      <td>3031.409836</td>\n",
       "      <td>4539.742081</td>\n",
       "      <td>3552.626070</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050989</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.327994</td>\n",
       "      <td>0.050173</td>\n",
       "      <td>0.520835</td>\n",
       "      <td>0.475589</td>\n",
       "      <td>0.005293</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.026749</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.348813</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.387821</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148</td>\n",
       "      <td>14</td>\n",
       "      <td>505</td>\n",
       "      <td>0.509018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.514851</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>849.0</td>\n",
       "      <td>252.5</td>\n",
       "      <td>13300</td>\n",
       "      <td>843300</td>\n",
       "      <td>227766.666667</td>\n",
       "      <td>2412.157937</td>\n",
       "      <td>5601.000000</td>\n",
       "      <td>3645.899929</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>21900</td>\n",
       "      <td>14650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.688242</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.211755</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.552040</td>\n",
       "      <td>0.188742</td>\n",
       "      <td>0.055446</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284722</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>294</td>\n",
       "      <td>14</td>\n",
       "      <td>274</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.937956</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>843300</td>\n",
       "      <td>210720.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3573.192408</td>\n",
       "      <td>1365.758157</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040010</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.040007</td>\n",
       "      <td>0.839153</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.269621</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        134              11               217         0.631579   \n",
       "1        415              11              1041         0.489423   \n",
       "2        625               9               486         0.599585   \n",
       "3        148              14               505         0.509018   \n",
       "4        294              14               274         0.620301   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.818966          4               2   \n",
       "1               1.0                  0.700321         22               3   \n",
       "2               1.0                  0.727273          4               3   \n",
       "3               1.0                  0.718861          8               4   \n",
       "4               1.0                  0.726190          5               1   \n",
       "\n",
       "   num_imgs  num_videos  average_token_length  num_keywords  \\\n",
       "0         2           0              4.488479             5   \n",
       "1         0          14              4.716619             5   \n",
       "2         1           0              4.862140             4   \n",
       "3         1           1              4.514851             6   \n",
       "4         1           0              4.937956             5   \n",
       "\n",
       "   data_channel_is_lifestyle  data_channel_is_entertainment  \\\n",
       "0                          0                              0   \n",
       "1                          0                              0   \n",
       "2                          1                              0   \n",
       "3                          0                              1   \n",
       "4                          0                              1   \n",
       "\n",
       "   data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  \\\n",
       "0                    0                       0                     1   \n",
       "1                    0                       0                     0   \n",
       "2                    0                       0                     0   \n",
       "3                    0                       0                     0   \n",
       "4                    0                       0                     0   \n",
       "\n",
       "   data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  \\\n",
       "0                      0          -1       591.0       174.6       16100   \n",
       "1                      0           4       417.0       116.4       68300   \n",
       "2                      0           4       343.0       247.5       36200   \n",
       "3                      0          -1       849.0       252.5       13300   \n",
       "4                      0          -1         0.0        -1.0           0   \n",
       "\n",
       "   kw_max_max     kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0      843300  334080.000000  2918.939394  3700.494673  3248.890010   \n",
       "1      843300  688300.000000  3396.488751  6678.414343  5181.926731   \n",
       "2      690400  349400.000000  3031.409836  4539.742081  3552.626070   \n",
       "3      843300  227766.666667  2412.157937  5601.000000  3645.899929   \n",
       "4      843300  210720.000000     0.000000  3573.192408  1365.758157   \n",
       "\n",
       "   self_reference_min_shares  self_reference_max_shares  \\\n",
       "0                    16100.0                      16100   \n",
       "1                     1400.0                       1900   \n",
       "2                     1300.0                       1300   \n",
       "3                     5500.0                      21900   \n",
       "4                     1100.0                       1100   \n",
       "\n",
       "   self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  \\\n",
       "0                     16100.0                  0                   0   \n",
       "1                      1650.0                  0                   1   \n",
       "2                      1300.0                  0                   1   \n",
       "3                     14650.0                  0                   0   \n",
       "4                      1100.0                  0                   0   \n",
       "\n",
       "   weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  \\\n",
       "0                     1                    0                  0   \n",
       "1                     0                    0                  0   \n",
       "2                     0                    0                  0   \n",
       "3                     1                    0                  0   \n",
       "4                     0                    1                  0   \n",
       "\n",
       "   weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01  \\\n",
       "0                    0                  0           0  0.040001  0.040001   \n",
       "1                    0                  0           0  0.040000  0.040021   \n",
       "2                    0                  0           0  0.050989  0.050009   \n",
       "3                    0                  0           0  0.033334  0.688242   \n",
       "4                    0                  0           0  0.040010  0.040826   \n",
       "\n",
       "     LDA_02    LDA_03    LDA_04  global_subjectivity  \\\n",
       "0  0.040001  0.040064  0.839932             0.380880   \n",
       "1  0.040000  0.839832  0.040147             0.564888   \n",
       "2  0.327994  0.050173  0.520835             0.475589   \n",
       "3  0.033334  0.211755  0.033335             0.552040   \n",
       "4  0.040007  0.839153  0.040005             0.269621   \n",
       "\n",
       "   global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0                   0.094661                    0.023041   \n",
       "1                   0.174474                    0.066282   \n",
       "2                   0.005293                    0.030864   \n",
       "3                   0.188742                    0.055446   \n",
       "4                   0.106667                    0.029197   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013825             0.625000             0.375000   \n",
       "1                    0.022094             0.750000             0.250000   \n",
       "2                    0.026749             0.535714             0.464286   \n",
       "3                    0.029703             0.651163             0.348837   \n",
       "4                    0.003650             0.888889             0.111111   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.367273               0.136364                    0.5   \n",
       "1               0.452530               0.050000                    1.0   \n",
       "2               0.348813               0.062500                    0.7   \n",
       "3               0.469600               0.100000                    1.0   \n",
       "4               0.350000               0.100000                    0.6   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.170370              -0.200000              -0.155556   \n",
       "1              -0.426268              -1.000000              -0.100000   \n",
       "2              -0.387821              -1.000000              -0.050000   \n",
       "3              -0.284722              -0.400000              -0.050000   \n",
       "4              -0.333333              -0.333333              -0.333333   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.288889                 -0.155556                0.211111   \n",
       "1            0.975000                  0.300000                0.475000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.000000                  0.000000                0.500000   \n",
       "\n",
       "   abs_title_sentiment_polarity  article_id  \n",
       "0                      0.155556           2  \n",
       "1                      0.300000           4  \n",
       "2                      0.000000          10  \n",
       "3                      0.000000          13  \n",
       "4                      0.000000          26  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score (XGBoost): 0.5558865\n",
      "Min Score (XGBoost): 0.012763874\n",
      "Max Score (Random Forest): 0.44596275804334057\n",
      "Min Score (Random Forest): 0.02271942992732906\n"
     ]
    }
   ],
   "source": [
    "# read the train and test datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# separate features and target variable in the training data\n",
    "X_train = train_df.drop(['is_popular', 'article_id', 'timedelta'], axis=1)\n",
    "y_train = train_df['is_popular']\n",
    "\n",
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(test_df.drop(['article_id', 'timedelta'], axis=1))\n",
    "\n",
    "\n",
    "xgb_params = {'max_depth': 3, 'n_estimators': 200, 'learning_rate': 0.05, 'subsample': 0.75, 'colsample_bytree': 0.75}\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=20240418, **xgb_params)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict probabilities for the test set using XGBoost model\n",
    "test_preds_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "\n",
    "rf_params = {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}\n",
    "rf_model = RandomForestClassifier(random_state=20240418, **rf_params)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict probabilities for the test set using Random Forest model\n",
    "test_preds_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# create a submissions DataFrame\n",
    "submissions_df_rf = pd.DataFrame({'article_id': test_df['article_id'], 'score': test_preds_rf})\n",
    "# create a submissions DataFrame\n",
    "submissions_df_xgb = pd.DataFrame({'article_id': test_df['article_id'], 'score': test_preds_xgb})\n",
    "\n",
    "# print max and min scores for XGBoost model\n",
    "print(\"Max Score (XGBoost):\", max(test_preds_xgb))\n",
    "print(\"Min Score (XGBoost):\", min(test_preds_xgb))\n",
    "\n",
    "# print max and min scores for Random Forest model\n",
    "print(\"Max Score (Random Forest):\", max(test_preds_rf))\n",
    "print(\"Min Score (Random Forest):\", min(test_preds_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score : 0.9759120826852845\n",
      "Min Score : 0.001273063672303064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.103990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.318283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.088312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.101559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>0.030642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id     score\n",
       "0           2  0.103990\n",
       "1           4  0.318283\n",
       "2          10  0.088312\n",
       "3          13  0.101559\n",
       "4          26  0.030642"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "# separate features and target in train data\n",
    "X_train = train_df.drop(['is_popular', 'article_id', 'timedelta'], axis=1)\n",
    "y_train = train_df['is_popular']\n",
    "\n",
    "# separate features in test data\n",
    "X_test = test_data[features]\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# make predictions on test data\n",
    "test_predictions = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# create a DataFrame for submission\n",
    "submission_df_logistic = pd.DataFrame({'article_id': test_data['article_id'], 'score': test_predictions})\n",
    "\n",
    "\n",
    "# print max and min scores\n",
    "print(\"Max Score :\", max(test_predictions))\n",
    "print(\"Min Score :\", min(test_predictions))\n",
    "\n",
    "\n",
    "submission_df_logistic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up the figure and axis\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# plot histogram for XGBoost model\n",
    "axes[0].hist(test_preds_xgb, bins=50, color='blue', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Scores (XGBoost)')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# plot histogram for Random Forest model\n",
    "axes[1].hist(test_preds_rf, bins=50, color='green', alpha=0.7)\n",
    "axes[1].set_title('Distribution of Scores (Random Forest)')\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# plot histogram for Logistic Regression model\n",
    "axes[2].hist(test_predictions, bins=50, color='red', alpha=0.7)\n",
    "axes[2].set_title('Distribution of Scores (Logistic Regression)')\n",
    "axes[2].set_xlabel('Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the results\n",
    "submissions_df_xgb.to_csv('sub_xgb.csv', index=False)\n",
    "submissions_df_rf.to_csv('sub_rf.csv', index=False)\n",
    "submission_df_logistic.to_csv('sub_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9911, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_df_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9911, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_df_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9911, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_logistic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.103990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.318283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.088312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.101559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>0.030642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id     score\n",
       "0           2  0.103990\n",
       "1           4  0.318283\n",
       "2          10  0.088312\n",
       "3          13  0.101559\n",
       "4          26  0.030642"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_logistic.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
